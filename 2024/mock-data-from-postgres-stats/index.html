<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><title data-next-head="">Using PostgreSQL production query planner statistics to fill a mock database - phiresky&#x27;s blog</title><meta name="description" content="Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner? Say you have a table users (id bigint, created timestamptz, category text): This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of " data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><link rel="alternate" type="application/rss+xml" title="RSS feed of phiresky&#x27;s blog" href="https://phiresky.github.io/blog/rss.xml" data-next-head=""/><link rel="alternate" type="application/atom+xml" title="Atom feed of phiresky&#x27;s blog" href="https://phiresky.github.io/blog/atom.xml" data-next-head=""/><link rel="alternate" type="application/json" title="JSON feed of phiresky&#x27;s blog" href="https://phiresky.github.io/blog/feed.json" data-next-head=""/><style data-next-head="">
          body {
            font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue,sans-serif;
          }
      </style><link rel="preload" href="/blog/_next/static/css/176ecc217408b6d5.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/176ecc217408b6d5.css" data-n-g=""/><link rel="preload" href="/blog/_next/static/css/ca399fb16b1ff7a4.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/ca399fb16b1ff7a4.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/blog/_next/static/chunks/webpack-b71e6f5e83c3e25f.js" defer=""></script><script src="/blog/_next/static/chunks/framework-dd2475e04127183a.js" defer=""></script><script src="/blog/_next/static/chunks/main-dded5530337538c6.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-4690c3d9b8989c63.js" defer=""></script><script src="/blog/_next/static/chunks/768-97507726c6ad4e38.js" defer=""></script><script src="/blog/_next/static/chunks/767-8dd3b40e4e29c204.js" defer=""></script><script src="/blog/_next/static/chunks/pages/%5Byear%5D/%5Bpost%5D-7f75829a52186db8.js" defer=""></script><script src="/blog/_next/static/HDgWMI_bt6Hpiw_tfDtBQ/_buildManifest.js" defer=""></script><script src="/blog/_next/static/HDgWMI_bt6Hpiw_tfDtBQ/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div><div><main class="lh-copy"><div class="relative tc bg-dark-gray"><div class="mw7 center white"><div class="pv4"><h1 class="f2 normal lh-title ma0 pa0"><a class="white no-underline" href="/blog/">phiresky&#x27;s blog</a></h1><h4 class="normal o-70 ma0 pt2 pb3 ph1">Code, Craft, and Creativity</h4><div><a class="dib f6 white no-underline pa1 ma1" href="/blog/">Blog</a><a href="https://github.com/phiresky/" class="dib f6 white no-underline pa1 ma1">GitHub</a></div></div></div></div><div class="content center mw7 pa3 pa4-ns"><h1 class="mt0 lh-title mb1">Using PostgreSQL production query planner statistics to fill a mock database</h1><small class="db ttu o-40"><time dateTime="2024-11-03T00:00:00.000Z">Nov 03, 2024</time></small><p>Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner?</p><p>Say you have a table <code class="not-highlighted">users (id bigint, created timestamptz, category text)</code>:</p><pre class="sql" style="color:#ccc;background:#2d2d2d;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-sql" style="white-space:pre;color:#ccc;background:none;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#cc99cd">analyze</span><span class="token token" style="color:#ccc">;</span><span>
</span><span></span><span class="token token" style="color:#cc99cd">select</span><span> </span><span class="token token" style="color:#67cdcc">*</span><span> </span><span class="token token" style="color:#cc99cd">from</span><span> pg_stats </span><span class="token token" style="color:#cc99cd">where</span><span> tablename </span><span class="token token" style="color:#67cdcc">=</span><span> </span><span class="token token" style="color:#7ec699">&#x27;users&#x27;</span><span class="token token" style="color:#ccc">;</span></code></pre><pre style="color:#ccc;background:#2d2d2d;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code style="white-space:pre;color:#ccc;background:none;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>schemaname             | public
</span>tablename              | users
<!-- -->attname                | created
<!-- -->null_frac              | 0
<!-- -->n_distinct             | -0.997816
<!-- -->most_common_vals       |
<!-- -->most_common_freqs      |
<!-- -->histogram_bounds       | {&quot;2024-01-01 00:12:56.0448+00&quot;,&quot;2024-01-04 18:19:55.344+00&quot;,...}
<!-- -->[...]
<!-- -->-----------------------
<!-- -->schemaname             | public
<!-- -->tablename              | users
<!-- -->attname                | category
<!-- -->null_frac              | 0
<!-- -->n_distinct             | 4561
<!-- -->most_common_vals       | {baz,foo,bar,slikmjelq,mltsglvtdc}
<!-- -->most_common_freqs      | {0.37733333,0.35873334,0.16336667,0.0878,0.015766667}
<!-- -->histogram_bounds       |
<!-- -->[...]</code></pre><p>This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of the stats:</p><figure><img src="./mock-data-visualization.png" title=""/><figcaption>Simple visualization of pg_stats of a mock table. Correlation refers to correlation between ascending order of columns to physical location on disk.</figcaption></figure><p>PostgreSQL by default looks at <a href="https://dba.stackexchange.com/a/200177/137516">30 thousand rows</a> of each table and then stores mainly two types of information for each column, depending on what it finds:</p><ol start="1" type="1"><li>The most common values, each with a percentage. (Only if some values are more common than others)</li><li>A histogram with equal-sized buckets to show the distribution of values (Only if not all values are present in the common vals list)</li></ol><p>In addition, it stores how many % of rows have null values and which fraction is unique. This is described in detail in the <a href="https://www.postgresql.org/docs/current/view-pg-stats.html">docs for pg_stats</a>.</p><p>One major limitation is that the statistics are completely separate per column. PG can also collect <a href="https://www.postgresql.org/docs/current/planner-stats.html#PLANNER-STATS-EXTENDED">multivariate statistics</a>, but this is not active by default so we’ll ignore it.</p><h2 id="generating-mock-rows">Generating mock rows</h2><p>This means that we can now generate mock data by just sampling from the distributions!</p><p>I <s>had AI write some code</s> spent time writing my own code with love to do this for me: <button class="footnote-button">note</button></p><p><a href="https://github.com/phiresky/postgresql-planner-stats-to-mock-data" class="auto-linked">https://github.com/phiresky/postgresql-planner-stats-to-mock-data</a></p><pre class="bash" style="color:#ccc;background:#2d2d2d;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-bash" style="white-space:pre;color:#ccc;background:none;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#999"># run against prod, writes a json file with the tables and stats per column</span><span>
</span><span>$ </span><span class="token token assign-left" style="color:#7ec699">PGPASSWORD</span><span class="token token" style="color:#67cdcc">=</span><span>xxx
</span><span>    </span><span class="token token" style="color:#f08d49">npm</span><span> run extract-postgresql-stats postgresql://user@production
</span>
<span>Processing public.users </span><span class="token token" style="color:#ccc">[</span><span class="token token" style="color:#f08d49">1974</span><span> ms</span><span class="token token" style="color:#ccc">]</span></code></pre><p>This will give you a JSON file as well as a readable summary per table that you could for example use to create smarter mock data generation code: <img src="output-example.png" title=""/></p><p>Then, to fill another database with similar data, run</p><pre class="bash" style="color:#ccc;background:#2d2d2d;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-bash" style="white-space:pre;color:#ccc;background:none;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>$ </span><span class="token token" style="color:#f08d49">npm</span><span> run insert-mock-data postgresql://user@staging
</span><span>Generating data </span><span class="token token" style="color:#cc99cd">for</span><span> public.users
</span><span>Table </span><span class="token token" style="color:#f08d49">1</span><span>/1 public.users: Inserted </span><span class="token token" style="color:#f08d49">2000</span><span>/137403 rows
</span><span>Table </span><span class="token token" style="color:#f08d49">1</span><span>/1 public.users: Inserted </span><span class="token token" style="color:#f08d49">4000</span><span>/137403 rows
</span><span>Table </span><span class="token token" style="color:#f08d49">1</span><span>/1 public.users: Inserted </span><span class="token token" style="color:#f08d49">6000</span><span>/137403 rows
</span><span>Table </span><span class="token token" style="color:#f08d49">1</span><span>/1 public.users: Inserted </span><span class="token token" style="color:#f08d49">8000</span><span>/137403 rows
</span><span></span><span class="token token" style="color:#ccc">[</span><span class="token token" style="color:#ccc">..</span><span>.</span><span class="token token" style="color:#ccc">]</span><span>
</span>Data generation completed successfully.</code></pre><p>For both scripts, you can specify which schemas, tables, and columns to exclude in a config file. In addition, you can specify the number of rows to generate as a fraction of the amount that is present in prod:</p><pre class="json" style="color:#ccc;background:#2d2d2d;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-json" style="white-space:pre;color:#ccc;background:none;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span class="token token" style="color:#ccc">{</span><span>
</span><span>    </span><span class="token token" style="color:#999">// see Config.ts</span><span>
</span><span>    </span><span class="token token" style="color:#f8c555">&quot;prodFraction&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#f08d49">0.1</span><span class="token token" style="color:#ccc">,</span><span>
</span><span>    </span><span class="token token" style="color:#f8c555">&quot;excluded&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#ccc">{</span><span>
</span><span>        </span><span class="token token" style="color:#f8c555">&quot;tables&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#ccc">[</span><span class="token token" style="color:#7ec699">&quot;private.data&quot;</span><span class="token token" style="color:#ccc">]</span><span class="token token" style="color:#ccc">,</span><span>
</span><span>        </span><span class="token token" style="color:#f8c555">&quot;columns&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#ccc">[</span><span>
</span><span>            </span><span class="token token" style="color:#ccc">{</span><span>
</span><span>                </span><span class="token token" style="color:#f8c555">&quot;column&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#7ec699">&quot;public.users.password_hash&quot;</span><span class="token token" style="color:#ccc">,</span><span>
</span><span>                </span><span class="token token" style="color:#f8c555">&quot;strategy&quot;</span><span class="token token" style="color:#67cdcc">:</span><span> </span><span class="token token" style="color:#7ec699">&quot;skip&quot;</span><span>
</span><span>            </span><span class="token token" style="color:#ccc">}</span><span>
</span><span>        </span><span class="token token" style="color:#ccc">]</span><span>
</span><span>    </span><span class="token token" style="color:#ccc">}</span><span>
</span><span></span><span class="token token" style="color:#ccc">}</span></code></pre><p>I encountered the following problems:</p><ul><li><b>Primary key + Unique constraints</b>: If you have an <code class="not-highlighted">id integer</code> column, the distribution of those will be a completely uniform distribution between min(id) and max(id). If you sample from this distribution, you will very quickly encounter a <span class="quoted">&quot;<!-- -->unique constraint failed<!-- -->&quot;</span> error. We can split this into two cases:<ul><li><b>autoincrement / serial / bigserial</b>: For these columns, we simply leave them out and let the DB generate them for us</li><li><b>unique constraints</b>: This is more tricky. My solution was to store each generated row in memory and then resample if it’s not unique. This is fine when collisions are unlikely, but if the value space is small it can cause issues.</li></ul></li><li><b>Foreign keys</b>: If you have tables referencing other tables, then you can’t just blindly generate the referencing keys. Instead, we build a dependency graph and sort the tables in topological order. Then, for each foreign key column we sample from the real previously created rows instead of the statistics.</li><li><b>Private data</b>: My goal here is to use real production statistics but use the results in dev environments. The statistics include real samples from the source tables. We can’t leak sensitive data like email addresses or password hashes, so I added a configuration with a list of columns to excluded and which values to use instead.<ul><li>I found these values by feeding my whole schema (4000 lines) into AI and letting it give me suggestions for columns that might be sensitive.</li></ul></li><li><b>CHECK constraints</b>: These are more difficult to solve, so I didn’t bother solving them in a general manner - the solution to the <span class="quoted">&quot;<!-- -->private data<!-- -->&quot;</span> issue also solves this.</li></ul><h2 id="alternatives-motivation">Alternatives / Motivation</h2><p>My main motivation here was that we have a large (10 TB, a few tables with 1-10 billion rows) production database, and our developers very often write DB migrations or queries that work perfectly fine in development (where each table has 10k rows max) but are impossible to make work on production.</p><p>For a developer, it’s difficult to learn all the details on when which index applies and especially when a migration locks/rewrites a table and doesn’t. This even changes between each PostgreSQL version. Just as an example, answer this question:</p><p>If you add a text column with a default on a table with a billion rows, does this lock and rewrite the table or no? What if you now change the default afterwards?</p><p>This is described in the docs: Adding a column with a default used to lock but doesn’t anymore (since PG12 or so), but the second one still locks. The PG docs are good, but it’s still better to verify with a real database - and quicker if you have it around anyways.</p><h3 id="alternative-1-generating-mock-data-from-your-application-code">Alternative 1: Generating mock data from your application code</h3><p>This would likely be the best solution, and we do have some code for this purpose. But it’s difficult to maintain it, you’d basically have to make adding code for this a requirement for every new feature. And by experience this data is often actually not that similar to real data, for example because you usually have very long-tail behaviour, with 5% of users doing 90% of actions.</p><h3 id="alternative-2-generating-rows-with-ai">Alternative 2: Generating rows with AI</h3><p>This is actually what I wanted to start with: Feed the schema into AI, let it write code to generate N rows. But then I realized the AI needs to see some real samples, so I added that. Then I also realized I can also feed it the statistics that PG already knows to make it better. At that point I realized that it’s much more robust to just take AI out of the loop and generate rows purely based on the statistics.</p><p>The best-case quality of the resulting rows would be better with AI (e.g. if it sees a field named <span class="quoted">&quot;<!-- -->email<!-- -->&quot;</span> it could generate realistic looking emails), but the fragility and worst-case quality makes it much more tedious to work with.</p><p>A prompt to do this based on the result of the first step is still in the repo.</p><h3 id="alternative-3-generating-data-purely-based-on-the-schema">Alternative 3: Generating data purely based on the schema</h3><p>This is of course possible, but you’ll miss out on a ton of realism. For example, a <code class="not-highlighted">text</code> column is very often actually more like a mix of <code class="not-highlighted">enum</code> and free form text, with a few very common values. An <code class="not-highlighted">integer</code> or <code class="not-highlighted">enum</code> is often <code class="not-highlighted">0</code>, <code class="not-highlighted">1</code> or <code class="not-highlighted">first_enum_value</code>. Often there’s columns that are <code class="not-highlighted">null</code> 100% of the time.</p><p>If you just sample randomly, you won’t get the same query plans as on production. Since getting realistic DB interaction was my main goal, using the real statistics greatly goes towards this goal.</p><h2 id="accuracy">Accuracy</h2><p>Remember that PG makes statistics by sampling during analyze. So make sure you run <code class="not-highlighted">analyze</code> before trying this, and I’d also recommend to <code class="not-highlighted">alter system set default_statistics_target = 1000</code> to 10x the sample size. Depending on how strict your application is, it might crash when seeing the mock data. My goal was to just be able to just have devs be able to try out migrations and realistic queries, but if you need your application running you might need custom code to generate data instead.</p></div><footer class="center w5 f6 tc mt4"><p><a href="https://github.com/phiresky/blog/blob/master/posts/2024/mock-data-from-postgres-stats.md">View post source on GitHub</a></p></footer><div></div></main></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"filename":"2024/mock-data-from-postgres-stats.md","frontmatter":{"csl":"../ieee-with-url.csl","date":"2024-11-03","hidden":true,"references":[],"title":"Using PostgreSQL production query planner statistics to fill a mock database","url2cite-link-output":"sup"},"preview":"Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner? Say you have a table users (id bigint, created timestamptz, category text): This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of ","content_ast":[{"t":"Para","c":[{"t":"Str","c":"Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner?"}]},{"t":"Para","c":[{"t":"Str","c":"Say you have a table "},{"t":"Code","c":[["",[],[]],"users (id bigint, created timestamptz, category text)"]},{"t":"Str","c":":"}]},{"t":"CodeBlock","c":[["",["sql"],[]],"analyze;\nselect * from pg_stats where tablename = 'users';"]},{"t":"CodeBlock","c":[["",[],[]],"schemaname             | public\ntablename              | users\nattname                | created\nnull_frac              | 0\nn_distinct             | -0.997816\nmost_common_vals       |\nmost_common_freqs      |\nhistogram_bounds       | {\"2024-01-01 00:12:56.0448+00\",\"2024-01-04 18:19:55.344+00\",...}\n[...]\n-----------------------\nschemaname             | public\ntablename              | users\nattname                | category\nnull_frac              | 0\nn_distinct             | 4561\nmost_common_vals       | {baz,foo,bar,slikmjelq,mltsglvtdc}\nmost_common_freqs      | {0.37733333,0.35873334,0.16336667,0.0878,0.015766667}\nhistogram_bounds       |\n[...]"]},{"t":"Para","c":[{"t":"Str","c":"This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of the stats:"}]},{"t":"Figure","c":[["",[],[]],[null,[{"t":"Plain","c":[{"t":"Str","c":"Simple visualization of pg_stats of a mock table. Correlation refers to correlation between ascending order of columns to physical location on disk."}]}]],[{"t":"Plain","c":[{"t":"Image","c":[["",[],[]],[{"t":"Str","c":"Simple visualization of pg_stats of a mock table. Correlation refers to correlation between ascending order of columns to physical location on disk."}],["./mock-data-visualization.png",""]]}]}]]},{"t":"Para","c":[{"t":"Str","c":"PostgreSQL by default looks at "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"30 thousand rows"}],["https://dba.stackexchange.com/a/200177/137516",""]]},{"t":"Str","c":" of each table and then stores mainly two types of information for each column, depending on what it finds:"}]},{"t":"OrderedList","c":[[1,{"t":"Decimal"},{"t":"Period"}],[[{"t":"Plain","c":[{"t":"Str","c":"The most common values, each with a percentage. (Only if some values are more common than others)"}]}],[{"t":"Plain","c":[{"t":"Str","c":"A histogram with equal-sized buckets to show the distribution of values (Only if not all values are present in the common vals list)"}]}]]]},{"t":"Para","c":[{"t":"Str","c":"In addition, it stores how many % of rows have null values and which fraction is unique. This is described in detail in the "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"docs for pg_stats"}],["https://www.postgresql.org/docs/current/view-pg-stats.html",""]]},{"t":"Str","c":"."}]},{"t":"Para","c":[{"t":"Str","c":"One major limitation is that the statistics are completely separate per column. PG can also collect "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"multivariate statistics"}],["https://www.postgresql.org/docs/current/planner-stats.html#PLANNER-STATS-EXTENDED",""]]},{"t":"Str","c":", but this is not active by default so we’ll ignore it."}]},{"t":"Header","c":[2,["generating-mock-rows",[],[]],[{"t":"Str","c":"Generating mock rows"}]]},{"t":"Para","c":[{"t":"Str","c":"This means that we can now generate mock data by just sampling from the distributions!"}]},{"t":"Para","c":[{"t":"Str","c":"I "},{"t":"Strikeout","c":[{"t":"Str","c":"had AI write some code"}]},{"t":"Str","c":" spent time writing my own code with love to do this for me: "},{"t":"Note","c":[{"t":"Para","c":[{"t":"Str","c":"Most of this code was written by AI, but I will say AI is a just a tool used by a human so it’s mine, right?"}]}]}]},{"t":"Para","c":[{"t":"Link","c":[["",["auto-linked"],[]],[{"t":"Str","c":"https://github.com/phiresky/postgresql-planner-stats-to-mock-data"}],["https://github.com/phiresky/postgresql-planner-stats-to-mock-data",""]]}]},{"t":"CodeBlock","c":[["",["bash"],[]],"# run against prod, writes a json file with the tables and stats per column\n$ PGPASSWORD=xxx\n    npm run extract-postgresql-stats postgresql://user@production\n\nProcessing public.users [1974 ms]"]},{"t":"Para","c":[{"t":"Str","c":"This will give you a JSON file as well as a readable summary per table that you could for example use to create smarter mock data generation code: "},{"t":"Image","c":[["",[],[]],[{"t":"Str","c":"output example text"}],["output-example.png",""]]}]},{"t":"Para","c":[{"t":"Str","c":"Then, to fill another database with similar data, run"}]},{"t":"CodeBlock","c":[["",["bash"],[]],"$ npm run insert-mock-data postgresql://user@staging\nGenerating data for public.users\nTable 1/1 public.users: Inserted 2000/137403 rows\nTable 1/1 public.users: Inserted 4000/137403 rows\nTable 1/1 public.users: Inserted 6000/137403 rows\nTable 1/1 public.users: Inserted 8000/137403 rows\n[...]\nData generation completed successfully."]},{"t":"Para","c":[{"t":"Str","c":"For both scripts, you can specify which schemas, tables, and columns to exclude in a config file. In addition, you can specify the number of rows to generate as a fraction of the amount that is present in prod:"}]},{"t":"CodeBlock","c":[["",["json"],[]],"{\n    // see Config.ts\n    \"prodFraction\": 0.1,\n    \"excluded\": {\n        \"tables\": [\"private.data\"],\n        \"columns\": [\n            {\n                \"column\": \"public.users.password_hash\",\n                \"strategy\": \"skip\"\n            }\n        ]\n    }\n}"]},{"t":"Para","c":[{"t":"Str","c":"I encountered the following problems:"}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Primary key + Unique constraints"}]},{"t":"Str","c":": If you have an "},{"t":"Code","c":[["",[],[]],"id integer"]},{"t":"Str","c":" column, the distribution of those will be a completely uniform distribution between min(id) and max(id). If you sample from this distribution, you will very quickly encounter a "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"unique constraint failed"}]]},{"t":"Str","c":" error. We can split this into two cases:"}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"autoincrement / serial / bigserial"}]},{"t":"Str","c":": For these columns, we simply leave them out and let the DB generate them for us"}]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"unique constraints"}]},{"t":"Str","c":": This is more tricky. My solution was to store each generated row in memory and then resample if it’s not unique. This is fine when collisions are unlikely, but if the value space is small it can cause issues."}]}]]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Foreign keys"}]},{"t":"Str","c":": If you have tables referencing other tables, then you can’t just blindly generate the referencing keys. Instead, we build a dependency graph and sort the tables in topological order. Then, for each foreign key column we sample from the real previously created rows instead of the statistics."}]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Private data"}]},{"t":"Str","c":": My goal here is to use real production statistics but use the results in dev environments. The statistics include real samples from the source tables. We can’t leak sensitive data like email addresses or password hashes, so I added a configuration with a list of columns to excluded and which values to use instead."}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Str","c":"I found these values by feeding my whole schema (4000 lines) into AI and letting it give me suggestions for columns that might be sensitive."}]}]]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"CHECK constraints"}]},{"t":"Str","c":": These are more difficult to solve, so I didn’t bother solving them in a general manner - the solution to the "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"private data"}]]},{"t":"Str","c":" issue also solves this."}]}]]},{"t":"Header","c":[2,["alternatives-motivation",[],[]],[{"t":"Str","c":"Alternatives / Motivation"}]]},{"t":"Para","c":[{"t":"Str","c":"My main motivation here was that we have a large (10 TB, a few tables with 1-10 billion rows) production database, and our developers very often write DB migrations or queries that work perfectly fine in development (where each table has 10k rows max) but are impossible to make work on production."}]},{"t":"Para","c":[{"t":"Str","c":"For a developer, it’s difficult to learn all the details on when which index applies and especially when a migration locks/rewrites a table and doesn’t. This even changes between each PostgreSQL version. Just as an example, answer this question:"}]},{"t":"Para","c":[{"t":"Str","c":"If you add a text column with a default on a table with a billion rows, does this lock and rewrite the table or no? What if you now change the default afterwards?"}]},{"t":"Para","c":[{"t":"Str","c":"This is described in the docs: Adding a column with a default used to lock but doesn’t anymore (since PG12 or so), but the second one still locks. The PG docs are good, but it’s still better to verify with a real database - and quicker if you have it around anyways."}]},{"t":"Header","c":[3,["alternative-1-generating-mock-data-from-your-application-code",[],[]],[{"t":"Str","c":"Alternative 1: Generating mock data from your application code"}]]},{"t":"Para","c":[{"t":"Str","c":"This would likely be the best solution, and we do have some code for this purpose. But it’s difficult to maintain it, you’d basically have to make adding code for this a requirement for every new feature. And by experience this data is often actually not that similar to real data, for example because you usually have very long-tail behaviour, with 5% of users doing 90% of actions."}]},{"t":"Header","c":[3,["alternative-2-generating-rows-with-ai",[],[]],[{"t":"Str","c":"Alternative 2: Generating rows with AI"}]]},{"t":"Para","c":[{"t":"Str","c":"This is actually what I wanted to start with: Feed the schema into AI, let it write code to generate N rows. But then I realized the AI needs to see some real samples, so I added that. Then I also realized I can also feed it the statistics that PG already knows to make it better. At that point I realized that it’s much more robust to just take AI out of the loop and generate rows purely based on the statistics."}]},{"t":"Para","c":[{"t":"Str","c":"The best-case quality of the resulting rows would be better with AI (e.g. if it sees a field named "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"email"}]]},{"t":"Str","c":" it could generate realistic looking emails), but the fragility and worst-case quality makes it much more tedious to work with."}]},{"t":"Para","c":[{"t":"Str","c":"A prompt to do this based on the result of the first step is still in the repo."}]},{"t":"Header","c":[3,["alternative-3-generating-data-purely-based-on-the-schema",[],[]],[{"t":"Str","c":"Alternative 3: Generating data purely based on the schema"}]]},{"t":"Para","c":[{"t":"Str","c":"This is of course possible, but you’ll miss out on a ton of realism. For example, a "},{"t":"Code","c":[["",[],[]],"text"]},{"t":"Str","c":" column is very often actually more like a mix of "},{"t":"Code","c":[["",[],[]],"enum"]},{"t":"Str","c":" and free form text, with a few very common values. An "},{"t":"Code","c":[["",[],[]],"integer"]},{"t":"Str","c":" or "},{"t":"Code","c":[["",[],[]],"enum"]},{"t":"Str","c":" is often "},{"t":"Code","c":[["",[],[]],"0"]},{"t":"Str","c":", "},{"t":"Code","c":[["",[],[]],"1"]},{"t":"Str","c":" or "},{"t":"Code","c":[["",[],[]],"first_enum_value"]},{"t":"Str","c":". Often there’s columns that are "},{"t":"Code","c":[["",[],[]],"null"]},{"t":"Str","c":" 100% of the time."}]},{"t":"Para","c":[{"t":"Str","c":"If you just sample randomly, you won’t get the same query plans as on production. Since getting realistic DB interaction was my main goal, using the real statistics greatly goes towards this goal."}]},{"t":"Header","c":[2,["accuracy",[],[]],[{"t":"Str","c":"Accuracy"}]]},{"t":"Para","c":[{"t":"Str","c":"Remember that PG makes statistics by sampling during analyze. So make sure you run "},{"t":"Code","c":[["",[],[]],"analyze"]},{"t":"Str","c":" before trying this, and I’d also recommend to "},{"t":"Code","c":[["",[],[]],"alter system set default_statistics_target = 1000"]},{"t":"Str","c":" to 10x the sample size. Depending on how strict your application is, it might crash when seeing the mock data. My goal was to just be able to just have devs be able to try out migrations and realistic queries, but if you need your application running you might need custom code to generate data instead."}]}],"default":{"filename":"2024/mock-data-from-postgres-stats.md","frontmatter":{"csl":"../ieee-with-url.csl","date":"2024-11-03","hidden":true,"references":[],"title":"Using PostgreSQL production query planner statistics to fill a mock database","url2cite-link-output":"sup"},"preview":"Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner? Say you have a table users (id bigint, created timestamptz, category text): This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of ","content_ast":[{"t":"Para","c":[{"t":"Str","c":"Did you know that PostgreSQL stores statistics about your data that can also be pretty useful not just for the query planner?"}]},{"t":"Para","c":[{"t":"Str","c":"Say you have a table "},{"t":"Code","c":[["",[],[]],"users (id bigint, created timestamptz, category text)"]},{"t":"Str","c":":"}]},{"t":"CodeBlock","c":[["",["sql"],[]],"analyze;\nselect * from pg_stats where tablename = 'users';"]},{"t":"CodeBlock","c":[["",[],[]],"schemaname             | public\ntablename              | users\nattname                | created\nnull_frac              | 0\nn_distinct             | -0.997816\nmost_common_vals       |\nmost_common_freqs      |\nhistogram_bounds       | {\"2024-01-01 00:12:56.0448+00\",\"2024-01-04 18:19:55.344+00\",...}\n[...]\n-----------------------\nschemaname             | public\ntablename              | users\nattname                | category\nnull_frac              | 0\nn_distinct             | 4561\nmost_common_vals       | {baz,foo,bar,slikmjelq,mltsglvtdc}\nmost_common_freqs      | {0.37733333,0.35873334,0.16336667,0.0878,0.015766667}\nhistogram_bounds       |\n[...]"]},{"t":"Para","c":[{"t":"Str","c":"This information is a bit hard to interpret, so let’s just let AI give us a nice visualization of the stats:"}]},{"t":"Figure","c":[["",[],[]],[null,[{"t":"Plain","c":[{"t":"Str","c":"Simple visualization of pg_stats of a mock table. Correlation refers to correlation between ascending order of columns to physical location on disk."}]}]],[{"t":"Plain","c":[{"t":"Image","c":[["",[],[]],[{"t":"Str","c":"Simple visualization of pg_stats of a mock table. Correlation refers to correlation between ascending order of columns to physical location on disk."}],["./mock-data-visualization.png",""]]}]}]]},{"t":"Para","c":[{"t":"Str","c":"PostgreSQL by default looks at "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"30 thousand rows"}],["https://dba.stackexchange.com/a/200177/137516",""]]},{"t":"Str","c":" of each table and then stores mainly two types of information for each column, depending on what it finds:"}]},{"t":"OrderedList","c":[[1,{"t":"Decimal"},{"t":"Period"}],[[{"t":"Plain","c":[{"t":"Str","c":"The most common values, each with a percentage. (Only if some values are more common than others)"}]}],[{"t":"Plain","c":[{"t":"Str","c":"A histogram with equal-sized buckets to show the distribution of values (Only if not all values are present in the common vals list)"}]}]]]},{"t":"Para","c":[{"t":"Str","c":"In addition, it stores how many % of rows have null values and which fraction is unique. This is described in detail in the "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"docs for pg_stats"}],["https://www.postgresql.org/docs/current/view-pg-stats.html",""]]},{"t":"Str","c":"."}]},{"t":"Para","c":[{"t":"Str","c":"One major limitation is that the statistics are completely separate per column. PG can also collect "},{"t":"Link","c":[["",[],[]],[{"t":"Str","c":"multivariate statistics"}],["https://www.postgresql.org/docs/current/planner-stats.html#PLANNER-STATS-EXTENDED",""]]},{"t":"Str","c":", but this is not active by default so we’ll ignore it."}]},{"t":"Header","c":[2,["generating-mock-rows",[],[]],[{"t":"Str","c":"Generating mock rows"}]]},{"t":"Para","c":[{"t":"Str","c":"This means that we can now generate mock data by just sampling from the distributions!"}]},{"t":"Para","c":[{"t":"Str","c":"I "},{"t":"Strikeout","c":[{"t":"Str","c":"had AI write some code"}]},{"t":"Str","c":" spent time writing my own code with love to do this for me: "},{"t":"Note","c":[{"t":"Para","c":[{"t":"Str","c":"Most of this code was written by AI, but I will say AI is a just a tool used by a human so it’s mine, right?"}]}]}]},{"t":"Para","c":[{"t":"Link","c":[["",["auto-linked"],[]],[{"t":"Str","c":"https://github.com/phiresky/postgresql-planner-stats-to-mock-data"}],["https://github.com/phiresky/postgresql-planner-stats-to-mock-data",""]]}]},{"t":"CodeBlock","c":[["",["bash"],[]],"# run against prod, writes a json file with the tables and stats per column\n$ PGPASSWORD=xxx\n    npm run extract-postgresql-stats postgresql://user@production\n\nProcessing public.users [1974 ms]"]},{"t":"Para","c":[{"t":"Str","c":"This will give you a JSON file as well as a readable summary per table that you could for example use to create smarter mock data generation code: "},{"t":"Image","c":[["",[],[]],[{"t":"Str","c":"output example text"}],["output-example.png",""]]}]},{"t":"Para","c":[{"t":"Str","c":"Then, to fill another database with similar data, run"}]},{"t":"CodeBlock","c":[["",["bash"],[]],"$ npm run insert-mock-data postgresql://user@staging\nGenerating data for public.users\nTable 1/1 public.users: Inserted 2000/137403 rows\nTable 1/1 public.users: Inserted 4000/137403 rows\nTable 1/1 public.users: Inserted 6000/137403 rows\nTable 1/1 public.users: Inserted 8000/137403 rows\n[...]\nData generation completed successfully."]},{"t":"Para","c":[{"t":"Str","c":"For both scripts, you can specify which schemas, tables, and columns to exclude in a config file. In addition, you can specify the number of rows to generate as a fraction of the amount that is present in prod:"}]},{"t":"CodeBlock","c":[["",["json"],[]],"{\n    // see Config.ts\n    \"prodFraction\": 0.1,\n    \"excluded\": {\n        \"tables\": [\"private.data\"],\n        \"columns\": [\n            {\n                \"column\": \"public.users.password_hash\",\n                \"strategy\": \"skip\"\n            }\n        ]\n    }\n}"]},{"t":"Para","c":[{"t":"Str","c":"I encountered the following problems:"}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Primary key + Unique constraints"}]},{"t":"Str","c":": If you have an "},{"t":"Code","c":[["",[],[]],"id integer"]},{"t":"Str","c":" column, the distribution of those will be a completely uniform distribution between min(id) and max(id). If you sample from this distribution, you will very quickly encounter a "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"unique constraint failed"}]]},{"t":"Str","c":" error. We can split this into two cases:"}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"autoincrement / serial / bigserial"}]},{"t":"Str","c":": For these columns, we simply leave them out and let the DB generate them for us"}]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"unique constraints"}]},{"t":"Str","c":": This is more tricky. My solution was to store each generated row in memory and then resample if it’s not unique. This is fine when collisions are unlikely, but if the value space is small it can cause issues."}]}]]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Foreign keys"}]},{"t":"Str","c":": If you have tables referencing other tables, then you can’t just blindly generate the referencing keys. Instead, we build a dependency graph and sort the tables in topological order. Then, for each foreign key column we sample from the real previously created rows instead of the statistics."}]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"Private data"}]},{"t":"Str","c":": My goal here is to use real production statistics but use the results in dev environments. The statistics include real samples from the source tables. We can’t leak sensitive data like email addresses or password hashes, so I added a configuration with a list of columns to excluded and which values to use instead."}]},{"t":"BulletList","c":[[{"t":"Plain","c":[{"t":"Str","c":"I found these values by feeding my whole schema (4000 lines) into AI and letting it give me suggestions for columns that might be sensitive."}]}]]}],[{"t":"Plain","c":[{"t":"Strong","c":[{"t":"Str","c":"CHECK constraints"}]},{"t":"Str","c":": These are more difficult to solve, so I didn’t bother solving them in a general manner - the solution to the "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"private data"}]]},{"t":"Str","c":" issue also solves this."}]}]]},{"t":"Header","c":[2,["alternatives-motivation",[],[]],[{"t":"Str","c":"Alternatives / Motivation"}]]},{"t":"Para","c":[{"t":"Str","c":"My main motivation here was that we have a large (10 TB, a few tables with 1-10 billion rows) production database, and our developers very often write DB migrations or queries that work perfectly fine in development (where each table has 10k rows max) but are impossible to make work on production."}]},{"t":"Para","c":[{"t":"Str","c":"For a developer, it’s difficult to learn all the details on when which index applies and especially when a migration locks/rewrites a table and doesn’t. This even changes between each PostgreSQL version. Just as an example, answer this question:"}]},{"t":"Para","c":[{"t":"Str","c":"If you add a text column with a default on a table with a billion rows, does this lock and rewrite the table or no? What if you now change the default afterwards?"}]},{"t":"Para","c":[{"t":"Str","c":"This is described in the docs: Adding a column with a default used to lock but doesn’t anymore (since PG12 or so), but the second one still locks. The PG docs are good, but it’s still better to verify with a real database - and quicker if you have it around anyways."}]},{"t":"Header","c":[3,["alternative-1-generating-mock-data-from-your-application-code",[],[]],[{"t":"Str","c":"Alternative 1: Generating mock data from your application code"}]]},{"t":"Para","c":[{"t":"Str","c":"This would likely be the best solution, and we do have some code for this purpose. But it’s difficult to maintain it, you’d basically have to make adding code for this a requirement for every new feature. And by experience this data is often actually not that similar to real data, for example because you usually have very long-tail behaviour, with 5% of users doing 90% of actions."}]},{"t":"Header","c":[3,["alternative-2-generating-rows-with-ai",[],[]],[{"t":"Str","c":"Alternative 2: Generating rows with AI"}]]},{"t":"Para","c":[{"t":"Str","c":"This is actually what I wanted to start with: Feed the schema into AI, let it write code to generate N rows. But then I realized the AI needs to see some real samples, so I added that. Then I also realized I can also feed it the statistics that PG already knows to make it better. At that point I realized that it’s much more robust to just take AI out of the loop and generate rows purely based on the statistics."}]},{"t":"Para","c":[{"t":"Str","c":"The best-case quality of the resulting rows would be better with AI (e.g. if it sees a field named "},{"t":"Quoted","c":[{"t":"DoubleQuote"},[{"t":"Str","c":"email"}]]},{"t":"Str","c":" it could generate realistic looking emails), but the fragility and worst-case quality makes it much more tedious to work with."}]},{"t":"Para","c":[{"t":"Str","c":"A prompt to do this based on the result of the first step is still in the repo."}]},{"t":"Header","c":[3,["alternative-3-generating-data-purely-based-on-the-schema",[],[]],[{"t":"Str","c":"Alternative 3: Generating data purely based on the schema"}]]},{"t":"Para","c":[{"t":"Str","c":"This is of course possible, but you’ll miss out on a ton of realism. For example, a "},{"t":"Code","c":[["",[],[]],"text"]},{"t":"Str","c":" column is very often actually more like a mix of "},{"t":"Code","c":[["",[],[]],"enum"]},{"t":"Str","c":" and free form text, with a few very common values. An "},{"t":"Code","c":[["",[],[]],"integer"]},{"t":"Str","c":" or "},{"t":"Code","c":[["",[],[]],"enum"]},{"t":"Str","c":" is often "},{"t":"Code","c":[["",[],[]],"0"]},{"t":"Str","c":", "},{"t":"Code","c":[["",[],[]],"1"]},{"t":"Str","c":" or "},{"t":"Code","c":[["",[],[]],"first_enum_value"]},{"t":"Str","c":". Often there’s columns that are "},{"t":"Code","c":[["",[],[]],"null"]},{"t":"Str","c":" 100% of the time."}]},{"t":"Para","c":[{"t":"Str","c":"If you just sample randomly, you won’t get the same query plans as on production. Since getting realistic DB interaction was my main goal, using the real statistics greatly goes towards this goal."}]},{"t":"Header","c":[2,["accuracy",[],[]],[{"t":"Str","c":"Accuracy"}]]},{"t":"Para","c":[{"t":"Str","c":"Remember that PG makes statistics by sampling during analyze. So make sure you run "},{"t":"Code","c":[["",[],[]],"analyze"]},{"t":"Str","c":" before trying this, and I’d also recommend to "},{"t":"Code","c":[["",[],[]],"alter system set default_statistics_target = 1000"]},{"t":"Str","c":" to 10x the sample size. Depending on how strict your application is, it might crash when seeing the mock data. My goal was to just be able to just have devs be able to try out migrations and realistic queries, but if you need your application running you might need custom code to generate data instead."}]}]}}},"__N_SSG":true},"page":"/[year]/[post]","query":{"year":"2024","post":"mock-data-from-postgres-stats"},"buildId":"HDgWMI_bt6Hpiw_tfDtBQ","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>